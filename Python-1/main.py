import google.generativeai as genai
import os
from dotenv import load_dotenv
from google.generativeai.types import generation_types

def generate():
    # Load environment variables from .env file
    #here i have  stored my api key in  .env file
    #or else directly paste ur api key in the api_key variable
    load_dotenv()

    # Configure the API key. Ensure your .env file has KEY=YOUR_API_KEY
    # Also, check if the key is actually loaded.
    api_key = os.environ.get("KEY")
    if not api_key:
        print("Error: API key 'KEY' not found in environment variables. Please check your .env file.")
        return # Exit if no API key

    genai.configure(api_key=api_key)

    # Create the model
    model = genai.GenerativeModel(
        model_name="gemini-2.0-flash", # This is a valid model name
        system_instruction="You are \"Chef Auguste,\" a world-renowned professional chef with a passion for exquisite cuisine and teaching. Your primary goal is to share culinary knowledge and guide users through the art of cooking. Maintain a confident, encouraging, and sophisticated tone, like a seasoned expert.\n\nYour responsibilities include:\n1.  **Recipe Guidance:** Provide clear, detailed, and easy-to-follow recipes for a wide range of dishes, from classic to contemporary.\n2.  **Technique Explanation:** Break down complex cooking techniques into digestible steps, offering tips for mastery.\n3.  **Ingredient Expertise:** Offer advice on ingredient selection, seasonality, substitutions, and flavor pairings.\n4.  **Problem Solving:** Help troubleshoot common cooking issues and offer solutions.\n5.  **Inspiration:** Inspire culinary creativity and confidence in the kitchen.\n\nAlways use precise culinary terms where appropriate, but explain them if necessary. Inject your responses with a subtle sense of professionalism and a love for food.In the beginning start with asking if the user wants a quick recipe or a step by step detailed recipe"
    )
    print("Let's get cooking! üç≥üî™") # Updated welcome message
    print("Ask me anything or type 'exit' to quit.")

    # Initialize chat history as a list of dictionaries, each representing a message
    #history contains all the chat history of the conversation
    chat_history = []

    while True:
        # Get user input
        user_input = input("You: ")

        if user_input.lower() in ["exit", "quit", "bye"]:
            print("Thanks for chatting! Keep being curious! üåü")
            break

        try:
            # Start a chat session with the current history
            # This creates a new chat session with the accumulated history in each turn.
            chat = model.start_chat(history=chat_history)
            generation_types=genai.types.GenerationConfig(
                max_output_tokens=500,
                temperature=0.7
            )
            # Send the user's message
            response = chat.send_message(user_input,generation_config=generation_types)

            # Extract the text from the response
            output = response.text

            print(f"Chef: {output}")

            
            # 'parts' must be a list of dictionaries, each with a 'text' key
            #example of chat [{"text":"hi there"},{text:"how are you?}]
            #these are generated by the model we created we just store them
        #used to get previous chats and generate next responses
            chat_history.append({
                "role": "user",
                "parts": [{"text": user_input}] # Correct format: list of parts
            })
            chat_history.append({
                "role": "model",
                "parts": [{"text": output}] # Correct format: list of parts
            })
           

        except Exception as e:
            # More robust error handling
            print(f"chef: Uh oh! I ran into a bit of a snag. Error: {e}")
            print("chef: My apologies, it seems I'm having a brief technical difficulty. Please try again in a moment!")
            # Optionally, you might want to break or handle specific errors differently
            # For example, if it's a rate limit error, you might suggest waiting.

if __name__ == '__main__':
    generate()
